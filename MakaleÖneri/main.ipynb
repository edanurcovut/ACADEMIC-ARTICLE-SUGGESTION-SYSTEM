{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kütüphaneler başarıyla yüklendi.\n"
     ]
    }
   ],
   "source": [
    "# Requirements dosyasında belirtilen ve programda kullanılan kütüphanelerin yüklenmesi\n",
    "import subprocess\n",
    "\n",
    "# requirements.txt dosyasının adı\n",
    "requirements_file = \"requirements.txt\"\n",
    "\n",
    "# requirements.txt dosyasındaki kütüphaneleri yükleme\n",
    "try:\n",
    "    subprocess.run([\"pip\", \"install\", \"-r\", requirements_file])\n",
    "    print(\"Kütüphaneler başarıyla yüklendi.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hata oluştu: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sadi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bütün kullanılacak kütüphaneler burada import edilir\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# nltk kütüphanesindeki stopwords ve noktalama işaretleri listesi indirilir\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP ile metin önişleme fonksiyonu\n",
    "def text_processing(text):\n",
    "    stop_words = set(stopwords.words('english')) # stop words seçilir\n",
    "    word_tokens = word_tokenize(text) # metindeki kelimeler ayrıştırılır\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words] #stop words çıkarılır\n",
    "    stemmer = PorterStemmer() # noktalama işaretleri çıkarılır\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_text] \n",
    "    result = \" \".join(stemmed_words)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sadi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# daha önceden eğitilimiş scibert modeli yüklenir\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\") \n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scibert vektör alma fonksiyonu\n",
    "def get_scibert_vector(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scibert vektörlerini csv olarak kaydeden fonksiyon\n",
    "def save_scibert_embedding_as_csv(name,embeddings):\n",
    "\n",
    "    file_name = \"scibert_embeddings/\"+name+\".csv\"\n",
    "\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for embedding in embeddings:\n",
    "            embedding_str = \",\".join([str(value.item()) for value in embedding])\n",
    "            file.write(embedding_str + \"\\n\")\n",
    "\n",
    "    print(f\"Embeddings dosyası '{file_name}' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext ile vektör alma fonksiyonu\n",
    "def get_fastxt_vector(text):\n",
    "    # Metinleri tokenize etme\n",
    "    tokenized_texts = simple_preprocess(text)\n",
    "\n",
    "    # FastText modelini eğitme\n",
    "    model = FastText(tokenized_texts, window=5, min_count=1)\n",
    "\n",
    "    # Her bir metnin vektör temsilini alıyoruz\n",
    "    vector_embeddings = [model.wv[text] for text in tokenized_texts]\n",
    "    return vector_embeddings\n",
    "    # Her bir metnin vektör temsilini yazdırma\n",
    "    #for i, embedding in enumerate(vector_embeddings):\n",
    "        #print(f\"Metin {i+1} vektör temsili: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext vektörlerini csv olarak kaydeden fonksiyon\n",
    "def save_fast_embedding_as_csv(name,embeddings):\n",
    "\n",
    "    file_name = \"fasttext_embeddings/\"+name+\".csv\"\n",
    "\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for embedding in embeddings:\n",
    "            embedding_str = \",\".join([str(value.item()) for value in embedding])\n",
    "            file.write(embedding_str + \"\\n\")\n",
    "\n",
    "    print(f\"Embeddings dosyası '{file_name}' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings dosyası 'scibert_embeddings/100.csv' olarak kaydedildi.\n",
      "22\n",
      "Embeddings dosyası 'fasttext_embeddings/100.csv' olarak kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# tüm makalelerin vektörlerinin çıkarılıp csv olarak kaydedilmesi (NOT: tek seferlik çalıştırılır. Çok vakit almaktadır)\n",
    "# makalelerin bulunduğu klasörlerin yolu\n",
    "metin_klasor = \"Inspec/docsutf8\" \n",
    "key_klasor = \"Inspec/keys\"\n",
    "\n",
    "# Klasördeki dosyaları listele\n",
    "dosyalar = os.listdir(metin_klasor)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Her dosya için işlem yap\n",
    "for dosya in dosyalar:\n",
    "    \n",
    "    key = dosya[:-4]+'.key'\n",
    "    # Dosya yolunu oluştur\n",
    "    dosya_yolu = os.path.join(metin_klasor, dosya)\n",
    "    # Key yolunu oluştur\n",
    "    key_yolu = os.path.join(key_klasor, key)\n",
    "\n",
    "    # Dosyanın bir text dosyası olduğundan emin ol\n",
    "    if os.path.isfile(dosya_yolu) and dosya.endswith(\".txt\"):\n",
    "        # Dosyayı aç ve içeriğini oku\n",
    "        with open(dosya_yolu, \"r\", encoding=\"utf-8\") as dosya_objesi:\n",
    "            # Dosyanın içeriğini oku\n",
    "            dosya_icerigi = dosya_objesi.read()\n",
    "            stemmed_txt = text_processing(dosya_icerigi) # metin ön işleme \n",
    "            scibert_embedding = get_scibert_vector(stemmed_txt) # scibert vektörlerini çıkart\n",
    "            save_scibert_embedding_as_csv(dosya[:-4],scibert_embedding) # scibert vektörlerini csv olarak kaydet\n",
    "            \n",
    "            fast_embedding = get_fastxt_vector(stemmed_txt) # fasttext vektörlerini çıkart\n",
    "            save_fast_embedding_as_csv(dosya[:-4],fast_embedding) # fasttext vektörlerini csv olarak kaydet\n",
    "            #print(stemmed_txt)\n",
    "        #with open(key_yolu, \"r\", encoding=\"utf-8\") as key_objesi:\n",
    "            # Dosyanın içeriğini oku\n",
    "            #key_icerigi = key_objesi.read()\n",
    "            #print(key)\n",
    "            #print(key_icerigi)\n",
    "            #text_processing(key_icerigi)\n",
    "    #i += 1\n",
    "    #if i ==3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity ile kullanıcı profili ile makaleler arasında benzerlik hesaplama fonksiyonu\n",
    "def calculate_similarity(user_profile, article_vectors):\n",
    "    similarities = cosine_similarity(user_profile, article_vectors)\n",
    "    return similarities[0]\n",
    "\n",
    "# fasttext için başlangıç öneri fonksiyonu\n",
    "def recommend_articles_fasttext(user_profile_fasttext, article_vectors_fasttext, top_n=5):\n",
    "    similarities_fasttext = calculate_similarity(user_profile_fasttext, article_vectors_fasttext)\n",
    "    sorted_indices_fasttext = similarities_fasttext.argsort()[::-1][:top_n]\n",
    "    recommended_articles_fasttext = [(index, similarities_fasttext[index]) for index in sorted_indices_fasttext]\n",
    "    return recommended_articles_fasttext\n",
    "\n",
    "# fasttext için makale seçimi sonrası öneri fonksiyonu\n",
    "def recommend_article_2_article_fasttext(user_profile_fasttext, article_vectors_fasttext, top_n=5):\n",
    "    similarities_fasttext = []\n",
    "    for user_vector in user_profile_fasttext:\n",
    "        similarities = cosine_similarity(user_vector, article_vectors_fasttext)[0]\n",
    "        similarities_fasttext.append(similarities)\n",
    "    \n",
    "    similarities_fasttext = np.mean(similarities_fasttext, axis=0)\n",
    "    \n",
    "    sorted_indices_fasttext = similarities_fasttext.argsort()[::-1][:top_n]\n",
    "    recommended_articles_fasttext = [(index, similarities_fasttext[index]) for index in sorted_indices_fasttext]\n",
    "    return recommended_articles_fasttext\n",
    "\n",
    "# scibert için öneri fonksiyonu\n",
    "def recommend_articles_scibert(user_profile_scibert, article_vectors_scibert, top_n=5):\n",
    "    similarities_scibert = calculate_similarity(user_profile_scibert, article_vectors_scibert)\n",
    "    sorted_indices_scibert = similarities_scibert.argsort()[::-1][:top_n]\n",
    "    recommended_articles_scibert = [(index, similarities_scibert[index]) for index in sorted_indices_scibert]\n",
    "    return recommended_articles_scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kullanıcı ilgi alanlarının scibert vektörlerini çıkarıp ortalamasını alan fonksiyon\n",
    "def get_average_scibert(text_list):\n",
    "    vectors = []\n",
    "    for text in text_list:\n",
    "        stemmed_txt = text_processing(text)\n",
    "        vector = get_scibert_vector(stemmed_txt)\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    # Vektörlerin ortalamasını al\n",
    "    average_vector = torch.stack(vectors).mean(dim=0).detach().numpy()\n",
    "    \n",
    "    return average_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kullanıcı ilgi alanlarının fasttext vektörlerini çıkarıp ortalamasını alan fonksiyon\n",
    "def get_average_fastxt(text_list):\n",
    "    vectors = []\n",
    "    for text in text_list:\n",
    "        stemmed_txt = text_processing(text)\n",
    "        vector = get_fastxt_vector(stemmed_txt)\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    # Vektörlerin ortalamasını al\n",
    "    average_vector = np.mean(vectors, axis=0)\n",
    "    average_vector = [sum(sublist) / len(sublist) for sublist in zip(*vectors)]    \n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings_from_csv_folder(folder_path):\n",
    "    all_embeddings = []\n",
    "    file_names = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            embeddings, file_name = read_embeddings_from_csv(file_path)\n",
    "            all_embeddings.extend(embeddings)\n",
    "            for i in range(len(embeddings)):\n",
    "                file_names.append(file_name)\n",
    "    return all_embeddings, file_names\n",
    "\n",
    "def read_embeddings_from_csv(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            embeddings.append([float(value) for value in row])\n",
    "    return embeddings, os.path.basename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext ve scibert embedding dosyalarını okur\n",
    "folder_path = \"fasttext_embeddings\"\n",
    "article_vectors_fasttext, fasttext_file_names = read_embeddings_from_csv_folder(folder_path)\n",
    "\n",
    "folder_path = \"scibert_embeddings\"\n",
    "article_vectors_scibert, scibert_file_names = read_embeddings_from_csv_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n",
      "FastText ile öneriler:\n",
      "Makale 4.csv, Benzerlik: 0.5943362409095669\n",
      "Makale 271.csv, Benzerlik: 0.5943362409095669\n",
      "Makale 1990.csv, Benzerlik: 0.5943362409095669\n",
      "Makale 1990.csv, Benzerlik: 0.5943362409095669\n",
      "Makale 1990.csv, Benzerlik: 0.5943362409095669\n",
      "\n",
      "SciBERT ile öneriler:\n",
      "Makale 218.csv, Benzerlik: 0.7906550644234528\n",
      "Makale 1959.csv, Benzerlik: 0.7853924203847588\n",
      "Makale 1455.csv, Benzerlik: 0.7717146099828255\n",
      "Makale 1543.csv, Benzerlik: 0.771253393736498\n",
      "Makale 1736.csv, Benzerlik: 0.7646994766367001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sadi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# Örnek kullanıcı profili \n",
    "# kullanıcı ilgi alanları seçer\n",
    "user_profile = [\"Computer engineering\", \"system design\", \"network\"]\n",
    "\n",
    "# kullanıcı ilgi alanlarının scibert ve fasttext ile vektörleri hesaplanıp tüm vektörlerin ortalaması alınır\n",
    "user_profile_scibert = get_average_scibert(user_profile)           \n",
    "user_profile_fasttext = get_average_fastxt(user_profile)\n",
    "\n",
    "# FastText için 5 öneri sunulur\n",
    "print(\"FastText ile öneriler:\")\n",
    "for index, similarity in recommend_articles_fasttext(user_profile_fasttext, article_vectors_fasttext):\n",
    "    print(f\"Makale {fasttext_file_names[index]}, Benzerlik: {similarity}\")\n",
    "\n",
    "# SciBERT için 5 öneri sunulur\n",
    "print(\"\\nSciBERT ile öneriler:\")\n",
    "for index, similarity in recommend_articles_scibert(user_profile_scibert, article_vectors_scibert):\n",
    "    print(f\"Makale {scibert_file_names[index]}, Benzerlik: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText ile öneriler:\n",
      "Makale 344.csv, Benzerlik: 1.0000000000000004\n",
      "Makale 1708.csv, Benzerlik: 1.0000000000000004\n",
      "Makale 2160.csv, Benzerlik: 1.0000000000000004\n",
      "Makale 1686.csv, Benzerlik: 1.0000000000000004\n",
      "Makale 1708.csv, Benzerlik: 1.0000000000000004\n",
      "scibert ile öneriler:\n",
      "Makale 1016.csv, Benzerlik: 1.0000000000000016\n",
      "Makale 1016.csv, Benzerlik: 0.8898809300869547\n",
      "Makale 1008.csv, Benzerlik: 0.8851984271505704\n",
      "Makale 1012.csv, Benzerlik: 0.8839402051680088\n",
      "Makale 1004.csv, Benzerlik: 0.8819873122876638\n"
     ]
    }
   ],
   "source": [
    "# Eğer makale seçimi yapılırsa\n",
    "# seçilen makaleye benzer olan makale bulunup en yakın makale önerilir\n",
    "\n",
    "selected_article = \"4\"\n",
    "\n",
    "user_prefer_fast, file = read_embeddings_from_csv(\"fasttext_embeddings/\"+selected_article+\".csv\")\n",
    "recommended_fast_articles = []\n",
    "# FastText için 5 öneri sunulur\n",
    "print(\"FastText ile öneriler:\")\n",
    "for index, similarity in recommend_articles_fasttext(user_prefer_fast, article_vectors_fasttext):\n",
    "    print(f\"Makale {fasttext_file_names[index]}, Benzerlik: {similarity}\")\n",
    "    recommended_fast_articles.extend(fasttext_file_names[index])\n",
    "\n",
    "user_prefer_scibert, file = read_embeddings_from_csv(\"scibert_embeddings/\"+selected_article+\".csv\")\n",
    "\n",
    "# Scibert için 5 öneri sunulur\n",
    "print(\"scibert ile öneriler:\")\n",
    "for index, similarity in recommend_articles_scibert(user_prefer_scibert, article_vectors_scibert):\n",
    "    print(f\"Makale {fasttext_file_names[index]}, Benzerlik: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(selected_articles, recommended_articles, top_n=5):\n",
    "    # Seçilen makalelerin sayısı\n",
    "    selected_count = len(selected_articles)\n",
    "    \n",
    "    # Tavsiye edilen makalelerin sayısı\n",
    "    recommended_count = top_n\n",
    "    \n",
    "    # Doğru önerilen makalelerin sayısı\n",
    "    correct_count = sum(1 for article in recommended_articles[:top_n] if article in selected_articles)\n",
    "    \n",
    "    # Precision hesapla\n",
    "    precision = correct_count / recommended_count if recommended_count > 0 else 0\n",
    "    \n",
    "    # Recall hesapla\n",
    "    recall = correct_count / selected_count if selected_count > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Kullanıcı tarafından seçilen makaleler aşağuıdaki gibi olduğunu varsayalım\n",
    "selected_articles = [\"4\",\"271\"]\n",
    "\n",
    "precision, recall = calculate_precision_recall(selected_articles, recommended_fast_articles)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
